01. 파이토치 패키지의 기본 구성
02. 텐서 조작하기(Tensor Manipulation) 1
03. 텐서 조작하기(Tensor Manipulation) 2
04. 파이썬 클래스(class)


01. 파이토치 패키지의 기본 구성
1. torch
메인 네임스페이스입니다. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가집니다.
2. torch.autograd
자동 미분을 위한 함수들이 포함되어져 있습니다. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 'Function' 등이 포함되어져 있습니다.
3. torch.nn
신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다. 예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.
4. torch.optim
확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다.
5. torch.utils.data
SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어져 있습니다.
6. torch.onnx
ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷입니다.

02. 텐서 조작하기(Tensor Manipulation) 1
벡터, 행렬, 텐서의 개념에 대해서 이해하고, Numpy와 파이토치로 벡터, 행렬, 텐서를 다루는 방법에 대해서 이해합니다.

1. 벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor)
차원이 없는 값을 스칼라, 1차원으로 구성된 값을 벡터, 2차원으로 구성된 값을 행렬(Matrix), 3차원으로 구성된 값을 텐서(Tensor)
3차원 이상의 텐서는 그냥 다차원 행렬 또는 배열로 간주 (데이터사이언스 분야)
1) PyTorch Tensor Shape Convention : 행렬과 텐서의 크기를 표현할 때 다음과 같은 방법으로 표기
(1) 2D Tensor(Typical Simple Setting)
|t| = (Batch size, dim)
훈련 데이터 하나의 크기를 256 이라고 해봅시다. [3, 1, 2, 5, ...] 이런 숫자들의 나열이 256의 길이로 있다. 
훈련 데이터 하나 = 벡터의 차원은 256 입니다. 
이런 훈련 데이터의 개수가 3000 개라고 한다면, 현재 전체 훈련 데이터의 크기는 3,000 × 256 입니다. 행렬이니까 2D 텐서네요. 
3,000개를 1개씩 꺼내서 처리하는 것도 가능하지만 컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리합니다. 
3,000개에서 64개씩 꺼내서 처리한다고 한다면 이 때 batch size를 64라고 합니다. 
그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 (batch size × dim) = 64 × 256 입니다.
(2) 3D Tensor(Typical Computer Vision)
|t| = (batch size, width, height) - 비전 분야에서의 3차원 텐서
일반적으로 자연어 처리보다 비전 분야(이미지, 영상 처리)를 하시게 된다면 좀 더 복잡한 텐서를 다루게 됩니다. 
이미지라는 것은 가로, 세로라는 것이 존재합니다. 
그리고 여러 장의 이미지, 그러니까 batch size로 구성하게 되면 아래와 같이 3차원의 텐서가 됩니다.
(3) 3D Tensor(Typical Natural Language Processing) - NLP 분야에서의 3차원 텐서
|t| = (batch size, length, dim)
자연어 처리는 보통 (batch size, 문장 길이, 단어 벡터의 차원)이라는 3차원 텐서를 사용
* NLP 분야의 3D 텐서 예제로 이해하기 
4개의 문장으로 구성된 전체 훈련 데이터
[[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]
컴퓨터는 아직 이 상태로는 '나는 사과를 좋아해'가 단어가 1개인지 3개인지 이해하지 못합니다. 
우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠주어야 합니다.
[['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]
이제 훈련 데이터의 크기는 4 × 3의 크기를 가지는 2D 텐서입니다.
컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있습니데. 이제 각 단어를 벡터로 만들겁니다. 아래와 같이 단어를 3차원의 벡터로 변환했다고 하겠습니다.
'나는' = [0.1, 0.2, 0.9]
'사과를' = [0.3, 0.5, 0.1]
'바나나를' = [0.3, 0.5, 0.2]
'좋아해' = [0.7, 0.6, 0.5]
'싫어해' = [0.5, 0.6, 0.7]
위 기준을 따라서 훈련 데이터를 재구성하면 아래와 같습니다.
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
이제 훈련 데이터는 4 × 3 × 3의 크기를 가지는 3D 텐서입니다. 이제 batch size를 2로 해보겠습니다.
첫번째 배치 #1
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]
두번째 배치 #2
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
컴퓨터는 배치 단위로 가져가서 연산을 수행합니다. 
그리고 현재 각 배치의 텐서의 크기는 (2 × 3 × 3)입니다. 이는 (batch size, 문장 길이, 단어 벡터의 차원)의 크기입니다.

2. 넘파이로 텐서 만들기(벡터와 행렬 만들기)
1) 1D with Numpy
t = np.array([0., 1., 2., 3., 4., 5., 6.])
print(t, t.ndim, t.shape, t.dtype, t.itemsize, t.nbytes, t.size)
print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # 인덱스를 통한 원소 접근
print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1]) # [시작 번호 : 끝 번호]로 범위 지정을 통해 가져온다.
print('t[:2] t[3:]     = ', t[:2], t[3:]) # 시작 번호를 생략한 경우와 끝 번호를 생략한 경우
2) 2D with Numpyㅍ
t2 = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t2, t2.ndim, t2.shape, t2.dtype, t2.itemsize, t2.nbytes, t2.size)

3. 파이토치 텐서 선언하기(PyTorch Tensor Allocation)
import torch
1) 1D with PyTorch
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t, t.dim(), t.shape, t.dtype, t.size())
print(t[0], t[1], t[-1])  # 인덱스로 접근
print(t[2:5], t[4:-1])    # 슬라이싱
print(t[:2], t[3:])       # 슬라이싱
2) 2D with PyTorch
t2 = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
print(t2, t2.dim(), t2.shape, t2.dtype, t2.size())
print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.
3) 브로드캐스팅(Broadcasting)
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # [3] -> [3, 3] 브로드캐스팅
print(m1 + m2)
# 2 x 1 Vector + 1 x 2 Vector
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])
print(m1 + m2)

# 브로드캐스팅 과정에서 실제로 두 텐서는 아래와 같이 브로드캐스팅
[1, 2]
==> [[1, 2],
     [1, 2]]
[[3], [4]]
==> [[3, 3],
     [4, 4]]

4) 자주 사용되는 기능들
(1) 행렬곱셈과 곱셈의 차이(Matrix Multiplication Vs. Multiplication)
- 행렬곱셈(.matmul)
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1
- element-wise 곱셈
print(m1 * m2) # 2 x 2
print(m1.mul(m2))
2) 평균(Mean)
m = torch.FloatTensor([1, 2])
print(m.mean())
m2 = torch.FloatTensor([[1, 2], [3, 4]])
print(m2.mean()) # tensor(2.5000)
# dim=0이라는 것은 첫번째 차원을 의미, 인자로 dim은 해당 차원을 제거한다는 의미
# 1과 3의 평균, 2와 4의 평균
print(m2.mean(dim=0)) # tensor([2., 3.]) 
print(m2.mean(dim=1)) # 두번째 차원을 제거, 1과 3의 평균, 3과 4의 평균
print(m2.mean(dim=-1)) # 마지막 차원을 제거한다는 의미
t2 = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # 2 x 3
print(t2.mean(dim=0)) # 1과 4의 평균, 2와 5의 평균, 3와 6의 평균
print(t2.mean(dim=1)) # 1,2,3의 평균 4,5,6의 평균
3) 덧셈(Sum)
t1 = torch.FloatTensor([[1, 2], [3, 4]])
print(t1.sum()) # 단순히 원소 전체의 덧셈을 수행
print(t1.sum(dim=0)) # 행을 제거
print(t1.sum(dim=1)) # 열을 제거
print(t1.sum(dim=-1)) # 열을 제거
3) 최대(Max)와 아그맥스(ArgMax)
print(t1.max()) # Returns one value: max
print(t1.max(dim=0)) # Returns two values: max and argmax
# 3과 4의 인덱스는 [1, 1]입니다.

4. 행렬 곱셈(Maxtrix Multiplication)


5. 다른 오퍼레이션들(Other Basic Ops)


03. 텐서 조작하기(Tensor Manipulation) 2
4) 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경. 매우 중요함!!

04. 파이썬 클래스(class)


