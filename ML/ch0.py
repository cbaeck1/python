import pandas as pd
import numpy as np

# 1. 붓꽃iris 데이터셋
# 2. 두 개의 특성을 가진 forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋
# 3. 위스콘신 유방암 Wisconsin Breast Cancer 데이터셋
# 4. 회귀 분석용 실제 데이터셋으로는 보스턴 주택가격 Boston Housing 데이터셋
# 5. 선형 회귀(최소제곱법)을 위한 wave 데이터셋. n_samples = 40
# 6. 세 개의 클래스를 가진 간단한 blobs 데이터셋
# 7. scikit-learn에 구현된 나이브 베이즈 분류기는 GaussianNB, BernoulliNB, MultinomialNB 세가지
# 8. 메모리 가격 동향 데이터 셋 ram_prices
# 9. 두 개의 클래스를 가진 2차원 데이터셋 two_moons
# 10. 두 개의 클래스를 가진 2차원 데이터셋 make_handcrafted
# 11. 두 개의 클래스를 가진 2차원 데이터셋 make_circles
# 12. 뉴스그룹 데이터 newsdata
# 13. 동물트리
# 14. 날씨 정보와 축구 경기 여부에 대한 데이터
# 15. wine 
# 16. 고유얼굴(eigenface) people
# 17. 3가지 합성된 신호 데이터셋
# 18. 숫자 데이터셋의 샘플 이미지 digits 
# 19. 클리블랜드(Cleveland) 심장병 재단에서 제공한 작은 데이터셋
# 20. life

# 1. 히스토그램
# 2. 산점도
# 3. n_neighbors 변화에 따른 결정 경계 
# 4. n_neighbors 변화에 따른 훈련 정확도와 테스트 정확도
# 5. 특성공학
# 6. 학습곡선
# 7. 결정 트리 분석 
# 8. 

# I. 지도학습
# 1. k-최근접 이웃 알고리즘 : 분류, 회귀
#    작은 데이터셋일 경우, 기본 모델로서 좋고 설명하기 쉬움
# 2. 선형모델 : 최소제곱, 릿지, 라쏘, 선형분류모델(로지스틱, 서포트벡터머신), 다중분류 선형모델
#    첫 번째로 시도할 알고리즘. 대용량 데이터셋 가능. 고차원 데이터에 가능
# 3. 나이브 베이즈 분류기
#    클래스에서 특정 기능의 효과가 다른 기능과 독립적이라고 가정
#    분류만 가능. 선형 모델보다 훨씬 빠름. 대용량 데이터셋과 고차원 데이터에 가능. 선형 모델보다 덜 정확함
# 4. 결정트리
#    매우 빠름. 데이터 스케일 조정이 필요 없음. 시각화하기 좋고 설명하기 쉬움.
# 5. 결정트리 앙상블 
#  5.1 랜덤 포레스트
#      결정 트리 하나보다 거의 항상 좋은 성능을 냄. 매우 안정적이고 강력함. 
#      데이터 스케일 조정 필요 없음. 고차원 희소 데이터에는 잘 안 맞음.
#  5.2 그래디언트 부스팅
#      랜덤 포레스트보다 조금 더 성능이 좋음. 랜덤 포레스트보다 학습은 느리나 예측은 빠르고 메모리를 조금 사용. 
#      랜덤 포레스트보다 매개변수 튜닝이 많이 필요함.
# 6. 커널서포트벡터머신
#    비슷한 의미의 특성으로 이뤄진 중간 규모 데이터셋에 잘 맞음. 데이터 스케일 조정 필요. 매개변수에 민감
# 7. 신경망
#    특별히 대용량 데이터셋에서 매우 복잡한 모델을 만들 수 있음. 
#    매개변수 선택과 데이터 스케일에 민감. 큰 모델은 학습이 오래 걸림.

# II. 데이터 전처리 & 비지도학습
# 1. 스케일을 조정
#  1.1. StandardScaler 는 각 특성의 평균을 0 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 한다.
#  1.2. RobustScaler 는 특성들이 같은 스케일을 같게 된다는 통계적 측면에서는 비슷하지만, 중간값과 사분위값을 사용한다.
#    이런 방식 때문에 전체 데이터와 아주 동떨어진 데이터의 영향을 받지 않는다.(이런 데이터를 이상치라 한다, outlier)
#  1.3. MinmaxScaler 는 모든 특성이 정확하게 0과 1사이에 위치하도록 데이터를 변경한다.
#  1.4. Normalizer 는 벡터의 유클리디안 길이가 1의 되도록 데이터 포인트를 조정한다. 
#    (지름이 1인 구에 데이터 포인트를 투영한다.) 
#    이러한 정규하는 득성 벡터의 길이는 상관 없고 데이터의 방향이 중요할 때 많이 사용한다.
# 2. 범주형 변수
# 
#

# 5. 비지도 변환 (unsupervised transformation)
# 5.1. 차원축소 : 시각화를 위해 데이터셋을 2차원으로 변경
#    데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘.
#  5.1. 주성분 분석 (principal component analysis): 가장 간단하고 널리 사용하는 알고리즘
#  5.2. 비음수 행렬 분해 (non-negative matriz factorization): 특성 추출에 널리 사용
#  5.3. t-SNE 알고리즘 (t-distributed stochastic neighbor embedding): 2차원 산점도를 이용해 시각화 용도로 많이 사용
# 5.2. 군집 (clustering) : 데이터를 비슷한 것끼리 그룹으로 묶는 것
#  5.3. k-평균 군집
#    k-평균은 비교적 이해하기 쉽고 구현도 쉬울 뿐만 아니라 비교적 빠르기 때문에 가장 인기있는 군집 알고리즘이다.
#    k-평균은 대용량 데이터셋에도 잘 작동하지만, MinBatchKMeans도 제공한다.
#    무작위 초기화를 사용하여 알구리즘의 출력이 난수 초깃값에 따라 달라진다.
#    클러스터의 모양을 가정하고 있어서 활용범위가 비교적 제한적이다.
#  5.4. 병합 군집 ( agglomerative clustering )
#    k-평균 군집 알고리즘의 단점을 개선
#  5.5. DBSCAN 
#    k-평균 군집 알고리즘의 단점을 개선
 
# III.  모델평가
# 



'''
파이썬 라이브러리를 활용한 머신러닝 
  사이킷런 핵심 개발자가 쓴 머신러닝과 데이터 과학 실무서 

CHAPTER 1 소개
1.1 왜 머신러닝인가?
__1.1.1 머신러닝으로 풀 수 있는 문제
__1.1.2 문제와 데이터 이해하기
1.2 왜 파이썬인가?
1.3 scikit-learn
__1.3.1 scikit-learn 설치
1.4 필수 라이브러리와 도구들
__1.4.1 주피터 노트북
__1.4.2 NumPy
__1.4.3 SciPy
__1.4.4 matplotlib
__1.4.5 pandas
__1.4.6 mglearn
1.5 파이썬 2 vs. 파이썬 3
1.6 이 책에서 사용하는 소프트웨어 버전
1.7 첫 번째 애플리케이션: 붓꽃의 품종 분류
__1.7.1 데이터 적재
__1.7.2 성과 측정: 훈련 데이터와 테스트 데이터
__1.7.3 가장 먼저 할 일: 데이터 살펴보기
__1.7.4 첫 번째 머신러닝 모델: k-최근접 이웃 알고리즘
__1.7.5 예측하기
__1.7.6 모델 평가하기
1.8 요약 및 정리

CHAPTER 2 지도 학습
2.1 분류와 회귀
2.2 일반화, 과대적합, 과소적합
__2.2.1 모델 복잡도와 데이터셋 크기의 관계
2.3 지도 학습 알고리즘
__2.3.1 예제에 사용할 데이터셋
__2.3.2 k-최근접 이웃
__2.3.3 선형 모델
__2.3.4 나이브 베이즈 분류기
__2.3.5 결정 트리
__2.3.6 결정 트리의 앙상블
__2.3.7 (한국어판 부록) 배깅, 엑스트라 트리, 에이다부스트
__2.3.8 커널 서포트 벡터 머신
__2.3.9 신경망(딥러닝)
2.4 분류 예측의 불확실성 추정
__2.4.1 결정 함수
__2.4.2 예측 확률
__2.4.3 다중 분류에서의 불확실성
2.5 요약 및 정리

CHAPTER 3 비지도 학습과 데이터 전처리
3.1 비지도 학습의 종류
3.2 비지도 학습의 도전 과제
3.3 데이터 전처리와 스케일 조정
__3.3.1 여러 가지 전처리 방법
__3.3.2 데이터 변환 적용하기
__3.3.3 (한국어판 부록) QuantileTransformer와 PowerTransformer
__3.3.4 훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기
__3.3.5 지도 학습에서 데이터 전처리 효과
3.4 차원 축소, 특성 추출, 매니폴드 학습
__3.4.1 주성분 분석(PCA)
__3.4.2 비음수 행렬 분해(NMF)
__3.4.3 t-SNE를 이용한 매니폴드 학습
3.5 군집
__3.5.1 k-평균 군집
__3.5.2 병합 군집
__3.5.3 DBSCAN
__3.5.4 군집 알고리즘의 비교와 평가
__3.5.5 군집 알고리즘 요약
3.6 요약 및 정리

CHAPTER 4 데이터 표현과 특성 공학
4.1 범주형 변수
__4.1.1 원-핫-인코딩(가변수)
__4.1.2 숫자로 표현된 범주형 특성
4.2 OneHotEncoder와 ColumnTransformer: scikit-learn으로 범주형 변수 다루기
4.3 make_column_transformer로 간편하게 ColumnTransformer 만들기
4.4 구간 분할, 이산화 그리고 선형 모델, 트리 모델
4.5 상호작용과 다항식
4.6 일변량 비선형 변환
4.7 특성 자동 선택
__4.7.1 일변량 통계
__4.7.2 모델 기반 특성 선택
__4.7.3 반복적 특성 선택
4.8 전문가 지식 활용
4.9 요약 및 정리

CHAPTER 5 모델평가와 성능 향상
5.1 교차 검증
__5.1.1 scikit-learn의 교차 검증
__5.1.2 교차 검증의 장점
__5.1.3 계층별 k-겹 교차 검증과 그외 전략들
__5.1.4 (한국어판 부록) 반복 교차 검증
5.2 그리드 서치
__5.2.1 간단한 그리드 서치
__5.2.2 매개변수 과대적합과 검증 세트
__5.2.3 교차 검증을 사용한 그리드 서치
5.3 평가 지표와 측정
__5.3.1 최종 목표를 기억하라
__5.3.2 이진 분류의 평가 지표
__5.3.3 다중 분류의 평가 지표
__5.3.4 회귀의 평가 지표
__5.3.5 모델 선택에서 평가 지표 사용하기
5.4 요약 및 정리

CHAPTER 6 알고리즘 체인과 파이프라인
6.1 데이터 전처리와 매개변수 선택
6.2 파이프라인 구축하기
6.3 그리드 서치에 파이프라인 적용하기
6.4 파이프라인 인터페이스
__6.4.1 make_pipleline을 사용한 파이프라인 생성
__6.4.2 단계 속성에 접근하기
__6.4.3 그리드 서치 안의 파이프라인 속성에 접근하기
6.5 전처리와 모델의 매개변수를 위한 그리드 서치
6.6 모델 선택을 위한 그리드 서치
__6.6.1 중복 계산 피하기
6.7 요약 및 정리

CHAPTER 7 텍스트 데이터 다루기
7.1 문자열 데이터 타입
7.2 예제 애플리케이션: 영화 리뷰 감성 분석
7.3 텍스트 데이터를 BOW로 표현하기
__7.3.1 샘플 데이터에 BOW 적용하기
__7.3.2 영화 리뷰에 대한 BOW
7.4 불용어
7.5 tf-idf로 데이터 스케일 변경하기
7.6 모델 계수 조사
7.7 여러 단어로 만든 BOW(n-그램)
7.8 고급 토큰화, 어간 추출, 표제어 추출
__7.8.1 (한국어판 부록) KoNLPy를 사용한 영화 리뷰 분석
7.9 토픽 모델링과 문서 군집화
__7.9.1 LDA
7.10 요약 및 정리

CHAPTER 8 마무리
8.1 머신러닝 문제 접근 방법
__8.1.1 의사 결정 참여
8.2 프로토타입에서 제품까지
8.3 제품 시스템 테스트
8.4 나만의 추정기 만들기
8.5 더 배울 것들
__8.5.1 이론
__8.5.2 다른 머신러닝 프레임워크와 패키지
__8.5.3 랭킹, 추천 시스템과 그 외 다른 알고리즘
__8.5.4 확률 모델링, 추론, 확률적 프로그래밍
__8.5.5 신경망
__8.5.6 대규모 데이터셋으로 확장
__8.5.7 실력 기르기
8.6 마치며
'''


